{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eng-Wola.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqtwz80mWUHhyAmxp0piAr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atnafuatx/atx/blob/master/Eng_Wola.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy6_RhZxBIeg"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XR_txB8BO10"
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\r\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\r\n",
        "import os\r\n",
        "source_language = \"en\"\r\n",
        "target_language = \"wol\"\r\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\r\n",
        "\r\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\r\n",
        "os.environ[\"tgt\"] = target_language\r\n",
        "os.environ[\"tag\"] = tag\r\n",
        "\r\n",
        "# This will save it to a folder in our gdrive instead!\r\n",
        "!mkdir -p \"/content/drive/My Drive/dawromodel/$src-$tgt\"\r\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/wolmodel/%s-%s\" % (source_language, target_language)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWjIqAEpBQ-a"
      },
      "source": [
        "from google.colab import files\r\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjz4klIfBSz6"
      },
      "source": [
        "import pandas as pd\r\n",
        "import io\r\n",
        "data = pd.read_csv(io.BytesIO(uploaded['eng-wolp.csv']), encoding='cp1252')\r\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiRjallwBY4f"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc983OkEBcqO"
      },
      "source": [
        "data[data.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv64vX-OBgr4"
      },
      "source": [
        "data = data.rename(columns={\"English\":\"source_sentence\", \"Dawuro\":\"target_sentence\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtH_SS_qBiQI"
      },
      "source": [
        "print(\"Length of Data before Removing duplicate: \",len(data))\r\n",
        "data = data.drop_duplicates()\r\n",
        "print(\"Length of Data after Removing duplicate: \",len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBJDpzTuBj2s"
      },
      "source": [
        "# Do the split between dev/test/train and create parallel corpora\r\n",
        "num_dev_patterns = 1000\r\n",
        "num_test_patterns = 1000\r\n",
        "df = data\r\n",
        "# Lower case the corpora\r\n",
        "df[\"source_sentence\"] = df[\"source_sentence\"].str.lower()\r\n",
        "df[\"target_sentence\"] = df[\"target_sentence\"].str.lower()\r\n",
        "\r\n",
        "\r\n",
        "devtest = df.tail(num_dev_patterns + num_test_patterns)\r\n",
        "test = devtest.tail(num_test_patterns)\r\n",
        "dev = devtest.head(num_dev_patterns)\r\n",
        "stripped = df.drop(df.tail(num_dev_patterns + num_test_patterns).index)\r\n",
        "\r\n",
        "stripped[[\"source_sentence\"]].to_csv(\"train.en\", index=False)\r\n",
        "stripped[[\"target_sentence\"]].to_csv(\"train.wol\", index=False)\r\n",
        "\r\n",
        "dev[[\"source_sentence\"]].to_csv(\"dev.en\", index=False)\r\n",
        "dev[[\"target_sentence\"]].to_csv(\"dev.wol\", index=False)\r\n",
        "\r\n",
        "test[[\"source_sentence\"]].to_csv(\"test.en\", index=False)\r\n",
        "test[[\"target_sentence\"]].to_csv(\"test.wol\", index=False)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27cr2z5yBmCF"
      },
      "source": [
        "\r\n",
        "# Install JoeyNMT\r\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\r\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f53l-SxHBob9"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \r\n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\r\n",
        "\r\n",
        "# Do subword NMT\r\n",
        "! mkdir joeynmt/data/\r\n",
        "! mkdir joeynmt/data/enwol/\r\n",
        "! export data_path=joeynmt/data/$src$tgt/\r\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\r\n",
        "\r\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\r\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\r\n",
        "\r\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\r\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\r\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\r\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\r\n",
        "\r\n",
        "# Create directory, move everyone we care about to the correct location\r\n",
        "#! mkdir -p $data_path\r\n",
        "! cp train.* joeynmt/data/enwol/\r\n",
        "! cp test.* joeynmt/data/enwol/\r\n",
        "! cp dev.* joeynmt/data/enwol/\r\n",
        "! cp bpe.codes.4000 $data_path\r\n",
        "! ls $data_path\r\n",
        "\r\n",
        "# Create that vocab using build_vocab\r\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\r\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\r\n",
        "\r\n",
        "# Some output\r\n",
        "! echo \"BPE Wolaita Sentences\"\r\n",
        "! tail -n 5 test.bpe.$tgt\r\n",
        "! echo \"Combined BPE Vocab\"\r\n",
        "! tail -n 10 joeynmt/data/enwol/vocab.txt\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahedl-OeBsO_"
      },
      "source": [
        "\r\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\r\n",
        "! cp train.* \"$gdrive_path\"\r\n",
        "! cp test.* \"$gdrive_path\"\r\n",
        "! cp dev.* \"$gdrive_path\"\r\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\r\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY23NwgQBxj1"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\r\n",
        "# (You can of course play with all the parameters if you'd like!)\r\n",
        "name = '%s%s' % (source_language, target_language)\r\n",
        "\r\n",
        "config = \"\"\"\r\n",
        "name: \"{name}_transformer\"\r\n",
        "\r\n",
        "data:\r\n",
        "    src: \"{source_language}\"\r\n",
        "    trg: \"{target_language}\"\r\n",
        "    train: \"data/{name}/train.bpe\"\r\n",
        "    dev:   \"data/{name}/dev.bpe\"\r\n",
        "    test:  \"data/{name}/test.bpe\"\r\n",
        "    level: \"bpe\"\r\n",
        "    lowercase: False\r\n",
        "    max_sent_length: 100\r\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\r\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\r\n",
        "\r\n",
        "testing:\r\n",
        "    beam_size: 5\r\n",
        "    alpha: 1.0\r\n",
        "\r\n",
        "training:\r\n",
        "    #load_model: \"models/{name}_transformer/12000.ckpt\" # if given, load a pre-trained model from this checkpoint\r\n",
        "    random_seed: 42\r\n",
        "    optimizer: \"adam\"\r\n",
        "    normalization: \"tokens\"\r\n",
        "    adam_betas: [0.9, 0.999] \r\n",
        "    scheduling: \"plateau\"            # Try switching from plateau to Noam scheduling\r\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\r\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\r\n",
        "    patience: 5   #8\r\n",
        "    decrease_factor: 0.7\r\n",
        "    loss: \"crossentropy\"\r\n",
        "    learning_rate: 0.0003\r\n",
        "    learning_rate_min: 0.00000001\r\n",
        "    weight_decay: 0.0\r\n",
        "    label_smoothing: 0.1\r\n",
        "    batch_size: 4096\r\n",
        "    batch_type: \"token\"\r\n",
        "    eval_batch_size: 3600\r\n",
        "    eval_batch_type: \"token\"\r\n",
        "    batch_multiplier: 1\r\n",
        "    early_stopping_metric: \"ppl\"\r\n",
        "    epochs: 10 #14  TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\r\n",
        "    validation_freq: 100 #400 Decrease this for testing\r\n",
        "    logging_freq: 100\r\n",
        "    eval_metric: \"bleu\"\r\n",
        "    model_dir: \"models/{name}_transformer\"\r\n",
        "    overwrite: True\r\n",
        "    shuffle: True\r\n",
        "    use_cuda: True\r\n",
        "    max_output_length: 100\r\n",
        "    print_valid_sents: [0, 1, 2, 3]\r\n",
        "    keep_last_ckpts: 3\r\n",
        "\r\n",
        "model:\r\n",
        "    initializer: \"xavier\"\r\n",
        "    bias_initializer: \"zeros\"\r\n",
        "    init_gain: 1.0\r\n",
        "    embed_initializer: \"xavier\"\r\n",
        "    embed_init_gain: 1.0\r\n",
        "    tied_embeddings: True\r\n",
        "    tied_softmax: True\r\n",
        "    encoder:\r\n",
        "        type: \"transformer\"\r\n",
        "        num_layers: 6\r\n",
        "        num_heads: 4 #TODO: Increase to 8 for larger data.\r\n",
        "        embeddings:\r\n",
        "            embedding_dim: 256 # TODO: Increase to 512 for larger data.\r\n",
        "            scale: True\r\n",
        "            dropout: 0.\r\n",
        "        # typically ff_size = 4 x hidden_size\r\n",
        "        hidden_size: 256   # TODO: Increase to 512 for larger data.\r\n",
        "        ff_size: 1024  # TODO: Increase to 2048 for larger data.\r\n",
        "        dropout: 0.3\r\n",
        "    decoder:\r\n",
        "        type: \"transformer\"\r\n",
        "        num_layers: 6\r\n",
        "        num_heads: 4 #8 TODO: Increase to 8 for larger data.\r\n",
        "        embeddings:\r\n",
        "            embedding_dim: 256 #512\r\n",
        "            scale: True\r\n",
        "            dropout: 0.\r\n",
        "        # typically ff_size = 4 x hidden_size\r\n",
        "        hidden_size: 256 #512\r\n",
        "        ff_size: 1024  #2048\r\n",
        "        dropout: 0.3\r\n",
        "\"\"\".format(name=name, source_language=source_language, target_language=target_language)\r\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\r\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sNPxMt_B0xM"
      },
      "source": [
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7dzVTd_B3Xm"
      },
      "source": [
        "! cat joeynmt/models/enom_transformer/validations.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzkY9wofB6Tc"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \r\n",
        "!mkdir \"$gdrive_path/models/\"\r\n",
        "!cp -r joeynmt/models/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7hypQgxB8Lv"
      },
      "source": [
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMki_8mPB-DI"
      },
      "source": [
        "! cd joeynmt; python3 -m joeynmt test models/enom_transformer/config.yaml"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}